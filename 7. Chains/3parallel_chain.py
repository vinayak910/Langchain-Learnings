# we get detailed doc on topic (ex : linear regression)
# we will generate 2 things 
# 1. NOTES 
# 2. QUIZ (combine and show to user)
from dotenv import load_dotenv

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel

from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from langchain_google_genai import ChatGoogleGenerativeAI


load_dotenv()

llm = HuggingFaceEndpoint(
    repo_id= "MiniMaxAI/MiniMax-M2.1",
    max_new_tokens=300,
    temperature=0.1
)

model1 = ChatHuggingFace(llm = llm)


model2 = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash-lite",
    temperature=0.2

)

prompt1 = PromptTemplate(
    template = "generate short and simple notes from the following text \n {text} ",
    input_variables=["text"]
)

prompt2 = PromptTemplate(
    template = "Generate 5 short question answers from the following text \n {text}",
    input_variables = ['text']
)

prompt3 = PromptTemplate(
    template ="Merge the provided notes and quiz into a single document \n "
    "notes->{notes} and quiz ->{quiz}",
    input_variables= ["notes", "quiz"]
)

parser = StrOutputParser()

# parallel chain -> runnable parallel 

parallel_chain = RunnableParallel(
    {
        'notes':prompt1 | model1 | parser,
        'quiz': prompt2 | model2 | parser
    }
    )

merge_chain = prompt3 | model2 | parser 

chain = parallel_chain | merge_chain

text = """
## Linear Regression â€” explained clearly, step by step

![Image](https://bookdown.org/dli/rguide/R-Manual_files/figure-html/unnamed-chunk-181-1.png)

![Image](https://atmos.uw.edu/~robwood/teaching/451/labs/images/xconcepts12.jpg.pagespeed.ic.oYghSsYO4w.jpg)

![Image](https://www.investopedia.com/thmb/00slg02wynhMgnRGGg8yhEYTNSA%3D/1500x0/filters%3Ano_upscale%28%29%3Amax_bytes%28150000%29%3Astrip_icc%28%29/LeastSquaresMethod-4eec23c588ce45ec9a771f1ce3abaf7f.jpg)

![Image](https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2017/05/FLP_bmi_sq.png?resize=576%2C384)

Think of **linear regression** as a very disciplined way of drawing the **best possible straight line** through data so that we can **understand relationships** and **make predictions**.

---

## 1ï¸âƒ£ What problem does linear regression solve?

You have data like:

* Area of a house â†’ Price
* Hours studied â†’ Marks
* Experience â†’ Salary

You want to answer:

> â€œIf I know **X**, can I reasonably predict **Y**?â€

Linear regression says:
ğŸ‘‰ *Yes, if the relationship is roughly linear.*

---

## 2ï¸âƒ£ The core idea (no math fear ğŸ˜„)

Linear regression assumes this form:

[
y = mx + c
]

Where:

* **x** â†’ input (feature)
* **y** â†’ output (target)
* **m** â†’ slope (how fast y changes when x changes)
* **c** â†’ intercept (value of y when x = 0)

ğŸ“Œ Example:

```
Price = 5000 Ã— Area + 2,00,000
```

This line is your **model**.

---

## 3ï¸âƒ£ What does â€œbest fit lineâ€ actually mean?

You already have real data points.
The line will **not** pass through all points.

So we define *best* as:

> The line that **minimizes the total error** between predicted and actual values.

### Error for one point:

[
error = actual_y - predicted_y
]

### Total error used by linear regression:

[
\textbf{Sum of Squared Errors (SSE)}
]

Why square?

* Removes negative signs
* Penalizes big mistakes more
* Smooth math for optimization

This method is called **Least Squares**.

---

## 4ï¸âƒ£ How does the model learn `m` and `c`?

Two main ways:

### âœ… Method 1: Closed-form solution (Normal Equation)

Direct math formula:
[
\theta = (X^TX)^{-1}X^Ty
]

Used when:

* Dataset is small
* Features are limited

### âœ… Method 2: Gradient Descent (most important)

Start with random values of `m` and `c`
Then repeatedly:

1. Predict y
2. Calculate error
3. Adjust parameters slightly
4. Repeat until error is minimum

This is **optimization**, not magic.

---

## 5ï¸âƒ£ Assumptions (very important for exams & interviews)

Linear regression works well **only if these roughly hold**:

1. **Linearity**
   Relationship between X and Y is linear

2. **Independence**
   Data points are independent

3. **Homoscedasticity**
   Error variance is constant

4. **Normality of errors**
   Errors roughly follow normal distribution

5. **No multicollinearity** (for multiple regression)
   Features shouldnâ€™t be highly correlated

âš ï¸ Violating assumptions â†’ misleading results

---

## 6ï¸âƒ£ Simple example (intuitive)

| Area (sqft) | Price (Lakh) |
| ----------- | ------------ |
| 800         | 40           |
| 1000        | 50           |
| 1200        | 60           |

The model learns:

```
Price â‰ˆ 0.05 Ã— Area
```

Now:

* Area = 1100
* Predicted price â‰ˆ **55 Lakh**

Thatâ€™s linear regression in action.

---

## 7ï¸âƒ£ Types of linear regression

### ğŸ”¹ Simple Linear Regression

* One feature
* `y = mx + c`

### ğŸ”¹ Multiple Linear Regression

* Many features
* `y = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + b`

Example:

```
Price = 4000Ã—Area + 5Ã—Age + 10000Ã—Location + c
```

---

## 8ï¸âƒ£ How do we judge if the model is good?

* **RÂ² score** â†’ how much variance is explained
* **MSE / RMSE** â†’ average prediction error
* **Residual plots** â†’ assumption checking

ğŸ“Œ High RÂ² + random residuals = good sign

---

## 9ï¸âƒ£ Where linear regression shines (and fails)

### âœ… Works great when:

* Relationship is roughly linear
* Data is clean & interpretable
* You need explainability

### âŒ Performs poorly when:

* Strong non-linearity
* Complex interactions
* Heavy outliers
      """

chain.get_graph().print_ascii()
result = chain.invoke({"text":text})

print(result)


